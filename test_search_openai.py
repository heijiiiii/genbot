import os
from dotenv import load_dotenv
from supabase import create_client
from langchain_cohere import CohereEmbeddings
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores.supabase import SupabaseVectorStore

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
COHERE_API_KEY = os.environ.get("COHERE_API_KEY", "")
SUPABASE_URL = os.environ.get("SUPABASE_URL", "")
SUPABASE_SERVICE_ROLE_KEY = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")

if not OPENAI_API_KEY:
    OPENAI_API_KEY = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

if not COHERE_API_KEY:
    COHERE_API_KEY = input("Cohere API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    os.environ["COHERE_API_KEY"] = COHERE_API_KEY

if not SUPABASE_URL:
    SUPABASE_URL = input("Supabase URLì„ ì…ë ¥í•˜ì„¸ìš”: ")
    os.environ["SUPABASE_URL"] = SUPABASE_URL

if not SUPABASE_SERVICE_ROLE_KEY:
    SUPABASE_SERVICE_ROLE_KEY = input("Supabase Service Role Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
    os.environ["SUPABASE_SERVICE_ROLE_KEY"] = SUPABASE_SERVICE_ROLE_KEY

# Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
try:
    client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)
    print("âœ… Supabase ì—°ê²° ì„±ê³µ!")
except Exception as e:
    print(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    exit(1)

# Cohere ì„ë² ë”© ì„¤ì •
try:
    cohere_embeddings = CohereEmbeddings(
        model="embed-v4.0",
        cohere_api_key=COHERE_API_KEY
    )
    print("âœ… Cohere ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì„±ê³µ!")
except Exception as e:
    print(f"âŒ Cohere ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
    exit(1)

# Supabase ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
try:
    text_vectorstore = SupabaseVectorStore(
        client=client,
        embedding=cohere_embeddings,
        table_name="text_embeddings",
        query_name="match_text_embeddings"
    )
    print("âœ… Supabase ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì„±ê³µ!")
except Exception as e:
    print(f"âŒ Supabase ë²¡í„° ìŠ¤í† ì–´ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
    exit(1)

# OpenAI LLM ì„¤ì •
try:
    llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0.2,
        api_key=OPENAI_API_KEY
    )
    print("âœ… OpenAI LLM ì„¤ì • ì„±ê³µ!")
except Exception as e:
    print(f"âŒ OpenAI LLM ì„¤ì • ì‹¤íŒ¨: {str(e)}")
    exit(1)

# ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_search(query):
    print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
    
    try:
        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        docs = text_vectorstore.similarity_search(query, k=3)
        
        if not docs:
            print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(docs)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!\n")
        
        # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
        for i, doc in enumerate(docs):
            print(f"[ê²°ê³¼ {i+1}]")
            print(f"ë‚´ìš©: {doc.page_content[:200]}{'...' if len(doc.page_content) > 200 else ''}")
            print(f"ë©”íƒ€ë°ì´í„°: í˜ì´ì§€={doc.metadata.get('page', 'ì—†ìŒ')}, ì¹´í…Œê³ ë¦¬={doc.metadata.get('category', 'ì—†ìŒ')}")
            print()
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        context = "\n".join([doc.page_content for doc in docs])
        prompt = f"""
        ë‹¹ì‹ ì€ ì œë„¤ì‹œìŠ¤ ì°¨ëŸ‰ ë§¤ë‰´ì–¼ì— íŠ¹í™”ëœ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”:
        
        ì°¸ê³  ì •ë³´:
        {context}
        
        ì§ˆë¬¸: {query}
        """
        
        response = llm.invoke(prompt)
        print("\nğŸ’¬ LLM ë‹µë³€:")
        print(response.content)
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
print("\n===== ì œë„¤ì‹œìŠ¤ ë§¤ë‰´ì–¼ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ =====")
print("(ì¢…ë£Œí•˜ë ¤ë©´ 'q' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”)")

while True:
    query = input("\nê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    if query.lower() in ['q', 'quit', 'ì¢…ë£Œ']:
        print("í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
        break
    
    test_search(query) 